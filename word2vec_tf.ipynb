{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('../large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "  return brown.sents()\n",
    "\n",
    "def get_brown(n_vocab=2000, keep_words = []):\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 0\n",
    "  word2idx = {}\n",
    "  idx2word = []\n",
    "\n",
    "  word_idx_count = {}\n",
    "\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        idx2word.append(token)\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      idx = word2idx[token]\n",
    "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "      indexed_sentence.append(idx)\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "  for word in keep_words:\n",
    "    word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  word2idx_small = {}\n",
    "  new_idx = 0\n",
    "  idx_new_idx_map = {}\n",
    "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "    word = idx2word[idx]\n",
    "    word2idx_small[word] = new_idx\n",
    "    idx_new_idx_map[idx] = new_idx\n",
    "    new_idx += 1\n",
    "  word2idx_small['UNKNOWN'] = new_idx \n",
    "  unknown = new_idx\n",
    "\n",
    "  sentences_small = []\n",
    "  for sentence in indexed_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "      sentences_small.append(new_sentence)\n",
    "\n",
    "  return sentences_small, word2idx_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences):\n",
    "  word_freq = {}\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          if word not in word_freq:\n",
    "              word_freq[word] = 0\n",
    "          word_freq[word] += 1\n",
    "  \n",
    "  V = len(word_freq)\n",
    "\n",
    "  p_neg = np.zeros(V)\n",
    "  for j in range(V):\n",
    "      p_neg[j] = word_freq[j]**0.75\n",
    "\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      context.append(ctx_word_idx)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(savedir):\n",
    "  sentences, word2idx = get_wiki()\n",
    "\n",
    "  vocab_size = len(word2idx)\n",
    "\n",
    "\n",
    "  # config\n",
    "  window_size = 10\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 \n",
    "  samples_per_epoch = int(1e5)\n",
    "  epochs = 20\n",
    "  D = 50\n",
    "\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  p_neg = get_negative_sampling_distribution(sentences)\n",
    "\n",
    "\n",
    "  W = np.random.randn(vocab_size, D).astype(np.float32) # input-to-hidden\n",
    "  V = np.random.randn(D, vocab_size).astype(np.float32) # hidden-to-output\n",
    "\n",
    "\n",
    "  tf_input = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "  tf_negword = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "  tf_context = tf.compat.v1.placeholder(tf.int32, shape=(None,)) # targets (context)\n",
    "  tfW = tf.Variable(W)\n",
    "  tfV = tf.Variable(V.T)2))\n",
    "\n",
    "  def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(input_tensor=C, axis=1)\n",
    "\n",
    "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_input) # 1 x D\n",
    "  emb_output = tf.nn.embedding_lookup(params=tfV, ids=tf_context) # N x D\n",
    "  correct_output = dot(emb_input, emb_output) # N\n",
    "  pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones(tf.shape(input=correct_output)), logits=correct_output)\n",
    "\n",
    "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_negword)\n",
    "  incorrect_output = dot(emb_input, emb_output)\n",
    "  neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.zeros(tf.shape(input=incorrect_output)), logits=incorrect_output)\n",
    "\n",
    "  loss = tf.reduce_mean(input_tensor=pos_loss) + tf.reduce_mean(input_tensor=neg_loss)\n",
    "\n",
    "\n",
    "  train_op = tf.compat.v1.train.MomentumOptimizer(0.1, momentum=0.9).minimize(loss)\n",
    "  session = tf.compat.v1.Session()\n",
    "  init_op = tf.compat.v1.global_variables_initializer()\n",
    "  session.run(init_op)\n",
    "\n",
    "  costs = []\n",
    "\n",
    "  total_words = sum(len(sentence) for sentence in sentences)\n",
    "  print(\"total number of words in corpus:\", total_words)\n",
    "\n",
    "\n",
    "  threshold = 1e-5\n",
    "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "\n",
    "\n",
    "  # train the model\n",
    "  for epoch in range(epochs):\n",
    "    np.random.shuffle(sentences)\n",
    "\n",
    "    cost = 0\n",
    "    counter = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    negwords = []\n",
    "    t0 = datetime.now()\n",
    "    for sentence in sentences:\n",
    "\n",
    "      sentence = [w for w in sentence \\\n",
    "        if np.random.random() < (1 - p_drop[w])\n",
    "      ]\n",
    "      if len(sentence) < 2:\n",
    "        continue\n",
    "\n",
    "\n",
    "      randomly_ordered_positions = np.random.choice(\n",
    "        len(sentence),\n",
    "        # size=np.random.randint(1, len(sentence) + 1),\n",
    "        size=len(sentence),\n",
    "        replace=False,\n",
    "      )\n",
    "\n",
    "\n",
    "      for j, pos in enumerate(randomly_ordered_positions):\n",
    "        word = sentence[pos]\n",
    "\n",
    "        context_words = get_context(pos, sentence, window_size)\n",
    "        neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "\n",
    "        \n",
    "        n = len(context_words)\n",
    "        inputs += [word]*n\n",
    "        negwords += [neg_word]*n\n",
    "        targets += context_words\n",
    "\n",
    "\n",
    "      if len(inputs) >= 128:\n",
    "        _, c = session.run(\n",
    "          (train_op, loss),\n",
    "          feed_dict={\n",
    "            tf_input: inputs,\n",
    "            tf_negword: negwords,\n",
    "            tf_context: targets,\n",
    "          }\n",
    "        )\n",
    "        cost += c\n",
    "\n",
    "        # reset\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        negwords = []\n",
    "\n",
    "      counter += 1\n",
    "      if counter % 100 == 0:\n",
    "        sys.stdout.write(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "        sys.stdout.flush()\n",
    "    dt = datetime.now() - t0\n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "\n",
    "\n",
    "    costs.append(cost)\n",
    "\n",
    "    learning_rate -= learning_rate_delta\n",
    "\n",
    "  W, VT = session.run((tfW, tfV))\n",
    "  V = VT.T\n",
    "\n",
    "  if not os.path.exists(savedir):\n",
    "    os.mkdir(savedir)\n",
    "\n",
    "  with open('%s/word2idx.json' % savedir, 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "  np.savez('%s/weights.npz' % savedir, W, V)\n",
    "\n",
    "  # return the model\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n",
      "total number of words in corpus: 86478677\n",
      "processed 270100 / 1271558\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-767db363a280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mword2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w2v_tf1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-85647a2662be>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(savedir)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtf_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mtf_negword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnegwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[0mtf_context\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m           }\n\u001b[0;32m    169\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1181\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "word2idx, W, V = train_model('w2v_tf1')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "  V, D = W.shape\n",
    "\n",
    "  # don't actually use pos2 in calculation, just print what's expected\n",
    "  print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "  for w in (pos1, neg1, pos2, neg2):\n",
    "    if w not in word2idx:\n",
    "      print(\"Sorry, %s not in word2idx\" % w)\n",
    "      return\n",
    "\n",
    "  p1 = W[word2idx[pos1]]\n",
    "  n1 = W[word2idx[neg1]]\n",
    "  p2 = W[word2idx[pos2]]\n",
    "  n2 = W[word2idx[neg2]]\n",
    "\n",
    "  vec = p1 - n1 + n2\n",
    "\n",
    "  distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "  idx = distances.argsort()[:10]\n",
    "\n",
    "  # pick one that's not p1, n1, or n2\n",
    "  best_idx = -1\n",
    "  keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    "  # print(\"keep_out:\", keep_out)\n",
    "  for i in idx:\n",
    "    if i not in keep_out:\n",
    "      best_idx = i\n",
    "      break\n",
    "  # print(\"best_idx:\", best_idx)\n",
    "\n",
    "  print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "  print(\"closest 10:\")\n",
    "  for i in idx:\n",
    "    print(idx2word[i], distances[i])\n",
    "\n",
    "  print(\"dist to %s:\" % pos2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "testing: france - french = england - english\n",
      "got: france - french = powerful - english\n",
      "closest 10:\n",
      "france 0.19890833\n",
      "powerful 0.46028405\n",
      "countries 0.51231515\n",
      "fat 0.54968715\n",
      "english 0.55031395\n",
      "trip 0.55916077\n",
      "dog 0.5900334\n",
      "others 0.61320984\n",
      "purpose 0.6287185\n",
      "got 0.62895656\n",
      "dist to england: 1.1183290854096413\n",
      "**********\n",
      "testing: france - french = england - english\n",
      "got: france - french = countries - english\n",
      "closest 10:\n",
      "france 0.3322333\n",
      "english 0.4039594\n",
      "countries 0.592022\n",
      "join 0.596961\n",
      "man's 0.6268618\n",
      "population 0.63550496\n",
      "sensitive 0.640213\n",
      "eight 0.66331506\n",
      "sun 0.6661253\n",
      "black 0.6668997\n",
      "dist to england: 1.0027547792997211\n"
     ]
    }
   ],
   "source": [
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "for We in (W, (W + V.T) / 2):\n",
    "  print(\"**********\")\n",
    "\n",
    "  analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
