{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp37-cp37m-win_amd64.whl (342.5 MB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\mazic\\\\appdata\\\\local\\\\programs\\\\python\\\\python37\\\\lib\\\\site-packages\\\\~cipy\\\\.libs\\\\libansari.R6EA3HQP5KZ6TAXU4Y4ZVTRPT7UVA53Z.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.31.0-cp37-cp37m-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorflow) (1.18.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting wheel>=0.26\n",
      "  Downloading wheel-0.35.1-py2.py3-none-any.whl (33 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9 MB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp37-cp37m-win_amd64.whl (1.0 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.21.1-py2.py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (47.1.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Using legacy setup.py install for termcolor, since package 'wheel' is not installed.\n",
      "Using legacy setup.py install for wrapt, since package 'wheel' is not installed.\n",
      "Installing collected packages: keras-preprocessing, grpcio, opt-einsum, absl-py, wheel, astunparse, idna, urllib3, chardet, certifi, requests, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, protobuf, tensorboard-plugin-wit, werkzeug, tensorboard, termcolor, scipy, google-pasta, tensorflow-estimator, h5py, gast, wrapt, tensorflow\n",
      "    Running setup.py install for termcolor: started\n",
      "    Running setup.py install for termcolor: finished with status 'done'\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.2\n",
      "    Uninstalling scipy-1.5.2:\n",
      "      Successfully uninstalled scipy-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "  \n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "          \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_small():\n",
    "  V = 20\n",
    "  file = 'large_files/enwiki-20180401-pages-articles1.xml-p10p30302-01.txt'\n",
    "  all_word_counts = {}\n",
    "  i = 0\n",
    "  for line in open(file, encoding=\"utf8\"):\n",
    "    if (i < 10): \n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          i += 1\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "            \n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "  \n",
    "  sents = []\n",
    "  i = 0\n",
    "  for line in open(file, encoding=\"utf8\"):\n",
    "    if (i < 10): \n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          i += 1\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = {}\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          if word not in word_freq:\n",
    "              word_freq[word] = 0\n",
    "          word_freq[word] += 1\n",
    "  \n",
    "  # vocab size\n",
    "  V = len(word_freq)\n",
    "\n",
    "  p_neg = np.zeros(V)\n",
    "  for j in range(V):\n",
    "      p_neg[j] = word_freq[j]**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      context.append(ctx_word_idx)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "  sentences, word2idx = get_wiki_small()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  # number of unique words\n",
    "  vocab_size = len(word2idx)\n",
    "\n",
    "\n",
    "  # config\n",
    "  window_size = 5\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 # number of negative samples to draw per input word\n",
    "  samples_per_epoch = int(1e5)\n",
    "  epochs = 20\n",
    "  D = 5 # word embedding size\n",
    "\n",
    "  # learning rate decay\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  # distribution for drawing negative samples\n",
    "  p_neg = get_negative_sampling_distribution(sentences)\n",
    "\n",
    "\n",
    "  # params\n",
    "  W = np.random.randn(vocab_size, D).astype(np.float32) # input-to-hidden\n",
    "  V = np.random.randn(D, vocab_size).astype(np.float32) # hidden-to-output\n",
    "\n",
    "\n",
    "  # create the model\n",
    "  tf_input = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "  tf_negword = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
    "  tf_context = tf.compat.v1.placeholder(tf.int32, shape=(None,)) # targets (context)\n",
    "  tfW = tf.Variable(W)\n",
    "  tfV = tf.Variable(V.T)\n",
    "  # biases = tf.Variable(np.zeros(vocab_size, dtype=np.float32))\n",
    "\n",
    "  def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(input_tensor=C, axis=1)\n",
    "\n",
    "  # correct middle word output\n",
    "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_input) # 1 x D\n",
    "  emb_output = tf.nn.embedding_lookup(params=tfV, ids=tf_context) # N x D\n",
    "  correct_output = dot(emb_input, emb_output) # N\n",
    "  # emb_input = tf.transpose(emb_input, (1, 0))\n",
    "  # correct_output = tf.matmul(emb_output, emb_input)\n",
    "  pos_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.ones(tf.shape(input=correct_output)), logits=correct_output)\n",
    "\n",
    "  # incorrect middle word output\n",
    "  emb_input = tf.nn.embedding_lookup(params=tfW, ids=tf_negword)\n",
    "  incorrect_output = dot(emb_input, emb_output)\n",
    "  # emb_input = tf.transpose(emb_input, (1, 0))\n",
    "  # incorrect_output = tf.matmul(emb_output, emb_input)\n",
    "  neg_loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=tf.zeros(tf.shape(input=incorrect_output)), logits=incorrect_output)\n",
    "\n",
    "  # total loss\n",
    "  loss = tf.reduce_mean(input_tensor=pos_loss) + tf.reduce_mean(input_tensor=neg_loss)\n",
    "\n",
    "  # output = hidden.dot(tfV)\n",
    "\n",
    "  # loss\n",
    "  # neither of the built-in TF functions work well\n",
    "  # per_sample_loss = tf.nn.nce_loss(\n",
    "  # # per_sample_loss = tf.nn.sampled_softmax_loss(\n",
    "  #   weights=tfV,\n",
    "  #   biases=biases,\n",
    "  #   labels=tfY,\n",
    "  #   inputs=hidden,\n",
    "  #   num_sampled=num_negatives,\n",
    "  #   num_classes=vocab_size,\n",
    "  # )\n",
    "  # loss = tf.reduce_mean(per_sample_loss)\n",
    "\n",
    "  # optimizer\n",
    "  # train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  train_op = tf.compat.v1.train.MomentumOptimizer(0.1, momentum=0.9).minimize(loss)\n",
    "  # train_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "  # make session\n",
    "  session = tf.compat.v1.Session()\n",
    "  init_op = tf.compat.v1.global_variables_initializer()\n",
    "  session.run(init_op)\n",
    "\n",
    "\n",
    "  # save the costs to plot them per iteration\n",
    "  costs = []\n",
    "\n",
    "\n",
    "  # number of total words in corpus\n",
    "  total_words = sum(len(sentence) for sentence in sentences)\n",
    "  print(\"total number of words in corpus:\", total_words)\n",
    "\n",
    "\n",
    "  # for subsampling each sentence\n",
    "  threshold = 1e-5\n",
    "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "\n",
    "\n",
    "  # train the model\n",
    "  for epoch in range(epochs):\n",
    "    # randomly order sentences so we don't always see\n",
    "    # sentences in the same order\n",
    "    np.random.shuffle(sentences)\n",
    "\n",
    "    # accumulate the cost\n",
    "    cost = 0\n",
    "    counter = 0\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    negwords = []\n",
    "    t0 = datetime.now()\n",
    "    for sentence in sentences:\n",
    "\n",
    "      # keep only certain words based on p_neg\n",
    "      sentence = [w for w in sentence \\\n",
    "        if np.random.random() < (1 - p_drop[w])\n",
    "      ]\n",
    "      if len(sentence) < 2:\n",
    "        continue\n",
    "\n",
    "\n",
    "      # randomly order words so we don't always see\n",
    "      # samples in the same order\n",
    "      randomly_ordered_positions = np.random.choice(\n",
    "        len(sentence),\n",
    "        # size=np.random.randint(1, len(sentence) + 1),\n",
    "        size=len(sentence),\n",
    "        replace=False,\n",
    "      )\n",
    "\n",
    "\n",
    "      for j, pos in enumerate(randomly_ordered_positions):\n",
    "        # the middle word\n",
    "        word = sentence[pos]\n",
    "\n",
    "        # get the positive context words/negative samples\n",
    "        context_words = get_context(pos, sentence, window_size)\n",
    "        neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "\n",
    "        \n",
    "        n = len(context_words)\n",
    "        inputs += [word]*n\n",
    "        negwords += [neg_word]*n\n",
    "        # targets = np.concatenate([targets, targets_])\n",
    "        targets += context_words\n",
    "\n",
    "        # _, c = session.run(\n",
    "        #   (train_op, loss),\n",
    "        #   feed_dict={\n",
    "        #     tf_input: [word],\n",
    "        #     tf_negword: [neg_word],\n",
    "        #     tf_context: targets_,\n",
    "        #   }\n",
    "        # )\n",
    "        # cost += c\n",
    "\n",
    "\n",
    "      if len(inputs) >= 128:\n",
    "        _, c = session.run(\n",
    "          (train_op, loss),\n",
    "          feed_dict={\n",
    "            tf_input: inputs,\n",
    "            tf_negword: negwords,\n",
    "            tf_context: targets,\n",
    "          }\n",
    "        )\n",
    "        cost += c\n",
    "\n",
    "        # reset\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        negwords = []\n",
    "\n",
    "      counter += 1\n",
    "      if counter % 100 == 0:\n",
    "        print(\"processed %s / %s\\r\" % (counter, len(sentences)))\n",
    "\n",
    "\n",
    "    # print stuff so we don't stare at a blank screen\n",
    "    dt = datetime.now() - t0\n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost, \"dt:\", dt)\n",
    "\n",
    "    # save the cost\n",
    "    costs.append(cost)\n",
    "\n",
    "    # update the learning rate\n",
    "    learning_rate -= learning_rate_delta\n",
    "\n",
    "\n",
    "  # get the params\n",
    "  W, VT = session.run((tfW, tfV))\n",
    "  V = VT.T\n",
    "\n",
    "  # return the model\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 3, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], [3, 9, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 9, 9, 9, 9, 9, 9, 9, 3, 9, 0, 9, 7, 9, 9, 9, 4, 9], [9, 9, 7, 0, 9, 9, 9, 3, 9, 9, 9, 9, 9, 9, 9, 2, 0, 9, 1, 9, 9, 9, 3, 9, 9, 9, 8, 9, 9, 4, 9, 1, 6, 9, 4, 6, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9], [3, 9, 9, 9, 8, 9, 9, 1, 9, 9, 8, 9, 9, 9, 9, 9, 9, 4, 9, 5, 8, 9, 9, 9, 4, 9, 1, 3, 9, 9, 9, 1, 9, 9, 9, 9, 6, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 7, 9, 9, 9, 1, 3, 9, 9, 9, 9, 9, 0, 9, 1, 9, 4, 9, 3, 9, 9, 9, 9], [0, 9, 3, 9, 9, 9, 0, 9, 9, 4, 0, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 4, 0, 9, 9, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 0, 9, 9, 9, 1, 9, 9, 9, 2, 9, 9, 9, 9, 0, 9, 9, 9, 9, 5, 9, 5, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 9, 9, 1, 0, 9, 9, 9, 9, 9, 7, 0, 6, 9, 1, 0, 9, 9, 9, 5, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 0, 9, 6, 9, 3, 2, 9, 9, 9, 9, 9], [0, 9, 9, 9, 7, 9, 9, 9, 6, 9, 9, 9, 9, 0, 9, 9, 1, 3, 2, 0, 9, 9, 9, 0, 9, 4, 9, 2, 9, 0, 9, 9, 9, 9, 9, 9, 5, 8, 9, 9, 3, 4, 9, 9, 9, 9, 2, 9, 9, 9, 0, 9, 2, 0, 9, 9, 9, 9, 9, 5, 8, 9, 9, 9, 9, 9, 0, 9, 9, 9, 0, 9, 9, 9, 9, 9, 7, 9, 7, 9, 9, 9, 9, 9, 9, 7, 9, 9, 3, 5, 9, 3], [0, 9, 6, 9, 9, 9, 9, 2, 0, 9, 9, 9, 9, 0, 9, 1, 9, 9, 9, 4, 2, 9, 9, 9, 9, 4, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 5, 6, 9, 9, 8, 9, 9, 9, 9, 2, 9, 8, 9, 9, 9, 8, 9, 1, 8, 9, 9, 1, 9, 4, 0, 9, 5, 9, 5, 9, 9, 9, 1, 9, 0, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 6, 2, 0, 9, 6, 9, 9, 9, 9, 0, 9, 9, 1, 9, 9, 9, 9, 4, 0, 9, 6, 9, 9, 9, 1, 0, 9, 2, 9, 9, 9, 9, 9, 1, 9, 9, 9, 9, 9, 0, 9, 9, 9, 9, 0, 9, 9, 0, 9, 9, 9, 0, 9, 9, 8, 9, 9, 9, 9, 9, 0, 9, 9], [0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 2, 9, 9, 9, 9, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 9, 0, 9, 9, 9, 9, 9, 4, 9, 9, 0, 9, 9, 9, 9, 9, 9, 5, 9, 6, 9, 9, 2, 0, 9, 9, 2, 9, 0, 9, 6, 9, 9, 0, 9, 9, 2, 9, 9, 0, 9, 9, 9, 5, 8, 9, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 0, 9, 1, 0, 9, 9, 9, 9, 5, 0, 9, 9, 9, 7, 9, 0, 9, 9, 2, 9, 7, 9, 9, 1, 9, 9, 9, 9, 5, 9, 9, 0, 9, 1, 0, 9, 9, 0, 9, 9, 3, 9, 9, 9, 9, 9, 9], [9, 3, 9, 9, 0, 9, 9, 9, 9, 1, 0, 9, 9, 9, 9, 9, 9, 0, 9, 9, 1, 9], [5, 9, 1, 0, 9, 9, 1, 0, 9, 2, 0, 9, 1, 0, 9, 9, 9, 9, 9, 0, 9, 9, 1, 9, 6, 9, 9, 7, 9, 9, 9, 9, 0, 9, 7, 9, 0, 9, 4, 9, 9, 1, 3, 9, 9, 9, 9, 9, 9, 9, 9, 7, 0, 9, 9, 2, 9, 9, 9, 9, 9, 9, 6, 9, 7, 9, 9, 9, 9]] {'the': 0, 'of': 1, 'in': 2, 'anarchism': 3, 'and': 4, 'as': 5, 'anarchist': 6, 'to': 7, 'a': 8, '<UNK>': 9}\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_wiki_small()\n",
    "print(sentences, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words in corpus: 784\n",
      "epoch complete: 0 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 1 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 2 cost: 0 dt: 0:00:00\n",
      "epoch complete: 3 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 4 cost: 0 dt: 0:00:00.000998\n",
      "epoch complete: 5 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 6 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 7 cost: 0 dt: 0:00:00.000998\n",
      "epoch complete: 8 cost: 0 dt: 0:00:00.001995\n",
      "epoch complete: 9 cost: 0 dt: 0:00:00.000998\n",
      "epoch complete: 10 cost: 0 dt: 0:00:00\n",
      "epoch complete: 11 cost: 0 dt: 0:00:00.000998\n",
      "epoch complete: 12 cost: 0 dt: 0:00:00\n",
      "epoch complete: 13 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 14 cost: 0 dt: 0:00:00.000998\n",
      "epoch complete: 15 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 16 cost: 0 dt: 0:00:00\n",
      "epoch complete: 17 cost: 0 dt: 0:00:00.000997\n",
      "epoch complete: 18 cost: 0 dt: 0:00:00\n",
      "epoch complete: 19 cost: 0 dt: 0:00:00.000986\n",
      "[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n",
      " [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]\n",
      " [ 0.14404356  1.4542735   0.7610377   0.12167501  0.44386324]\n",
      " [ 0.33367434  1.4940791  -0.20515826  0.3130677  -0.85409576]\n",
      " [-2.5529897   0.6536186   0.8644362  -0.742165    2.2697546 ]\n",
      " [-1.4543657   0.04575852 -0.18718386  1.5327792   1.4693588 ]\n",
      " [ 0.15494743  0.37816253 -0.88778573 -1.9807965  -0.34791216]\n",
      " [ 0.15634897  1.2302907   1.2023798  -0.3873268  -0.30230275]\n",
      " [-1.048553   -1.420018   -1.7062702   1.9507754  -0.5096522 ]\n",
      " [-0.4380743  -1.2527953   0.7774904  -1.6138978  -0.21274029]\n",
      " [-0.89546657  0.3869025  -0.51080513 -1.1806322  -0.02818223]\n",
      " [ 0.42833188  0.06651722  0.3024719  -0.6343221  -0.36274117]\n",
      " [-0.67246044 -0.35955316 -0.8131463  -1.7262826   0.17742614]\n",
      " [-0.40178093 -1.6301984   0.46278226 -0.9072984   0.0519454 ]\n",
      " [ 0.7290906   0.12898292  1.1394007  -1.2348258   0.40234163]\n",
      " [-0.6848101  -0.87079716 -0.5788497  -0.31155252  0.05616534]\n",
      " [-1.1651498   0.9008265   0.46566245 -1.5362437   1.4882522 ]\n",
      " [ 1.8958892   1.1787796  -0.17992483 -1.0707526   1.0544517 ]\n",
      " [-0.40317693  1.222445    0.20827498  0.97663903  0.3563664 ]\n",
      " [ 0.7065732   0.01050002  1.7858706   0.12691209  0.40198937]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "word2idx, W, V = train_model()\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79979108 0.33298967]\n",
      "[[0.16974153 0.56550604]\n",
      " [0.15930054 0.53072116]\n",
      " [0.00803347 0.02676408]\n",
      " [0.07554102 0.2516703 ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(6, 4)\n",
    "V = np.random.randn(4, 6)\n",
    "prob = sigmoid(W[3].dot(V[:,[2, 4]]))\n",
    "print(prob)\n",
    "\n",
    "gV = np.outer(W[3], prob - 1)\n",
    "print(gV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
