{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "from utils import find_analogies, get_wikipedia_data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = {}\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          if word not in word_freq:\n",
    "              word_freq[word] = 0\n",
    "          word_freq[word] += 1\n",
    "  \n",
    "  # vocab size\n",
    "  V = len(word_freq)\n",
    "\n",
    "  p_neg = np.zeros(V)\n",
    "  for j in range(V):\n",
    "      p_neg[j] = word_freq[j]**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      context.append(ctx_word_idx)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    # get the data\n",
    "  sentences, word2idx = get_wikipedia_data1(None, 2000) #get_text8()\n",
    "\n",
    "  # number of unique words\n",
    "  vocab_size = len(word2idx)\n",
    "\n",
    "  # config\n",
    "  window_size = 10\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 # number of negative samples to draw per input word\n",
    "  samples_per_epoch = int(1e5)\n",
    "  epochs = 20\n",
    "  D = 50 # word embedding size\n",
    "\n",
    "  # learning rate decay\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  # distribution for drawing negative samples\n",
    "  p_neg = get_negative_sampling_distribution(sentences)\n",
    "\n",
    "\n",
    "  total_words = sum(len(sentence) for sentence in sentences)\n",
    "  print(\"total number of words in corpus:\", total_words)\n",
    "\n",
    "\n",
    "  # for subsampling each sentence\n",
    "  threshold = 1e-5\n",
    "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "  \n",
    "  def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "  \n",
    "  inputs = []\n",
    "  contexts = []\n",
    "  labels = []\n",
    "  \n",
    "  print(\"dataset construction started ... \")\n",
    "\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    # keep only certain words based on p_neg\n",
    "    sentence = [w for w in sentence \\\n",
    "      if np.random.random() < (1 - p_drop[w])\n",
    "    ]\n",
    "    if len(sentence) < 2:\n",
    "      continue\n",
    "\n",
    "\n",
    "    # randomly order words so we don't always see\n",
    "    # samples in the same order\n",
    "    randomly_ordered_positions = np.random.choice(\n",
    "      len(sentence),\n",
    "      # size=np.random.randint(1, len(sentence) + 1),\n",
    "      size=len(sentence),\n",
    "      replace=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    for j, pos in enumerate(randomly_ordered_positions):\n",
    "      # the middle word\n",
    "      word = sentence[pos]\n",
    "      context_words = get_context(pos, sentence, window_size)\n",
    "      neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "      for target in context_words:\n",
    "        inputs.append(word)\n",
    "        contexts.append(target)\n",
    "        labels.append(1)\n",
    "        inputs.append(neg_word)\n",
    "        contexts.append(target)\n",
    "        labels.append(0)\n",
    "        \n",
    "  inputs = np.asarray(inputs)\n",
    "  contexts = np.asarray(contexts)\n",
    "  labels = np.asarray(labels, dtype='float32')\n",
    "  \n",
    "  return inputs, contexts, labels, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model(save_dir, inputs, contexts, labels, word2idx):\n",
    "  \n",
    "  print(inputs[0])\n",
    "  \n",
    "  print(\"training started ... \")\n",
    "  \n",
    "  vocab_size = len(word2idx)\n",
    "  D = 50\n",
    "  W = tf.Variable(tf.random.normal([vocab_size, D]))\n",
    "  V = tf.Variable(tf.random.normal([D, vocab_size])) \n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  epochs = 10\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  optimizer = tf.compat.v1.train.MomentumOptimizer(0.1, momentum=0.9) \n",
    "\n",
    "  def get_batch(batch_counter):\n",
    "    max_counter = len(labels)\n",
    "    return inputs[batch_counter:min(batch_counter + 100, max_counter),], contexts[batch_counter:min(batch_counter + 100, max_counter),], labels[batch_counter:min(batch_counter + 100, max_counter)]\n",
    "\n",
    "  costs = []\n",
    "  \n",
    "  # train the model\n",
    "  for epoch in range(epochs):\n",
    "    cost = 0\n",
    "    batch_counter = 0\n",
    "    while batch_counter < len(labels):\n",
    "      input_batch, context_batch, label_batch = get_batch(batch_counter)\n",
    "      batch_counter += 100\n",
    "      with tf.GradientTape() as t:\n",
    "        input_embeddings = tf.nn.embedding_lookup(W, input_batch) # (100, 2000) * (2000, 50) - > (100, 50) embedding vector for each input\n",
    "        context_embeddings = tf.nn.embedding_lookup(tf.transpose(V), context_batch) # (100, 50)\n",
    "        simlarity = tf.einsum(\"ij,ij->i\", input_embeddings, context_embeddings)\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          labels=label_batch, logits=simlarity))\n",
    "        \n",
    "        cost += loss\n",
    "\n",
    "        grads = t.gradient(loss, [W, V])\n",
    "        optimizer.apply_gradients(zip(grads,[W, V]))\n",
    "        \n",
    "        if batch_counter % 10000 == 0:\n",
    "          sys.stdout.write(\"processed %s / %s\\r\" % (batch_counter, len(labels)))\n",
    "          sys.stdout.flush()\n",
    "    \n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost)\n",
    "  \n",
    "  W, V = W.numpy(), V.numpy()\n",
    "    \n",
    "  if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "  with open('%s/word2idx.json' % save_dir, 'w') as f:\n",
    "    json.dump(word2idx, f)\n",
    "\n",
    "  np.savez('%s/weights.npz' % save_dir, W, V)\n",
    "  # return the model\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main(save_dir):\n",
    "#   inputs, contexts, labels, word2idx = construct_dataset()\n",
    "#   word2idx, W, V = train_model(save_dir, inputs, contexts, labels, word2idx)\n",
    "  \n",
    "#   with open('%s/word2idx.json' % save_dir) as f:\n",
    "#     word2idx = json.load(f)\n",
    "  \n",
    "#   W, U = train(save_dir, 100)\n",
    "\n",
    "  with open('%s/word2idx.json' % save_dir) as f:\n",
    "    word2idx = json.load(f)\n",
    "  npz = np.load('%s/weights.npz' % save_dir)\n",
    "  W = npz['arr_0']\n",
    "  V = npz['arr_1']\n",
    "\n",
    "  idx2word = {i:w for w, i in word2idx.items()}\n",
    "  We = (W + V.T) / 2\n",
    "\n",
    "  find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "  find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "  find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "  find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "  find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "  find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "  find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "  find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "  find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest match by euclidean distance: queen\n",
      "king - man = queen - woman\n",
      "closest match by cosine distance: queen\n",
      "king - man = queen - woman\n",
      "closest match by euclidean distance: england\n",
      "france - paris = england - london\n",
      "closest match by cosine distance: wales\n",
      "france - paris = wales - london\n",
      "closest match by euclidean distance: italy\n",
      "france - paris = italy - rome\n",
      "closest match by cosine distance: italy\n",
      "france - paris = italy - rome\n",
      "closest match by euclidean distance: continued\n",
      "paris - france = continued - italy\n",
      "closest match by cosine distance: poland\n",
      "paris - france = poland - italy\n",
      "closest match by euclidean distance: england\n",
      "france - french = england - english\n",
      "closest match by cosine distance: england\n",
      "france - french = england - english\n",
      "closest match by euclidean distance: china\n",
      "japan - japanese = china - chinese\n",
      "closest match by cosine distance: china\n",
      "japan - japanese = china - chinese\n",
      "closest match by euclidean distance: italy\n",
      "japan - japanese = italy - italian\n",
      "closest match by cosine distance: italy\n",
      "japan - japanese = italy - italian\n",
      "closest match by euclidean distance: australia\n",
      "japan - japanese = australia - australian\n",
      "closest match by cosine distance: australia\n",
      "japan - japanese = australia - australian\n",
      "closest match by euclidean distance: july\n",
      "december - november = july - june\n",
      "closest match by cosine distance: july\n",
      "december - november = july - june\n"
     ]
    }
   ],
   "source": [
    "main('word2vec_wiki_2000_full')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
