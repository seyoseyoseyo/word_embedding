{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.data.path.append(\"C:\\\\UBS\\\\Dev\\\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!conda install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "  \n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "          \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_small():\n",
    "  V = 20\n",
    "  file = 'large_files/enwiki-20180401-pages-articles1.xml-p10p30302-01.txt'\n",
    "  all_word_counts = {}\n",
    "  i = 0\n",
    "  for line in open(file, encoding=\"utf8\"):\n",
    "    if (i < 10): \n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          i += 1\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "            \n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "  \n",
    "  sents = []\n",
    "  i = 0\n",
    "  for line in open(file, encoding=\"utf8\"):\n",
    "    if (i < 10): \n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          i += 1\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_brown_small():\n",
    "  sentences = brown.sents()\n",
    "  word_count = {}\n",
    "  \n",
    "  V = 20\n",
    "  i = 0\n",
    "\n",
    "  for sentence in sentences:\n",
    "    if i < 10:\n",
    "      i += 1\n",
    "      for word in sentence:\n",
    "        if word in word_count:\n",
    "          word_count[word] += 1\n",
    "        else:\n",
    "          word_count[word] = 1\n",
    "\n",
    "  all_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w: i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "  \n",
    "  sents = []\n",
    "  \n",
    "  i = 0\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    if i < 10:\n",
    "      i += 1\n",
    "      sent = [word2idx[word] if word in word2idx else unk for word in sentence]\n",
    "      sents.append(sent)\n",
    "\n",
    "  return sents, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = {}\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          if word not in word_freq:\n",
    "              word_freq[word] = 0\n",
    "          word_freq[word] += 1\n",
    "  \n",
    "  # vocab size\n",
    "  V = len(word_freq)\n",
    "\n",
    "  p_neg = np.zeros(V)\n",
    "  for j in range(V):\n",
    "      p_neg[j] = word_freq[j]**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      context.append(ctx_word_idx)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "  sentences, word2idx = get_brown_small()\n",
    "  p_neg = get_negative_sampling_distribution(sentences)\n",
    "  \n",
    "  # number of unique words\n",
    "  vocab_size = len(word2idx)\n",
    "\n",
    "\n",
    "  # config\n",
    "  window_size = 5\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 # number of negative samples to draw per input word\n",
    "  samples_per_epoch = int(1e5)\n",
    "  epochs = 5000\n",
    "  D = 5 # word embedding size\n",
    "\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  # distribution for drawing negative sample\n",
    "  # params\n",
    "  W = tf.Variable(tf.random.normal([vocab_size, D]))\n",
    "  V = tf.Variable(tf.random.normal([D, vocab_size])) # hidden-to-output\n",
    "\n",
    "\n",
    "  def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(input_tensor=C, axis=1)\n",
    "  \n",
    "  optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "  \n",
    "  x_train = []\n",
    "  y_train = []\n",
    "  \n",
    "  def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    for pos in range(len(sentence)):\n",
    "      word = sentence[pos]\n",
    "      context_words = get_context(pos, sentence, window_size)\n",
    "      for target in context_words:\n",
    "        x_train.append(to_one_hot(word, vocab_size))\n",
    "        y_train.append(to_one_hot(target, vocab_size))\n",
    "        \n",
    "  x_train = np.asarray(x_train, dtype='float32')\n",
    "  y_train = np.asarray(y_train, dtype='float32')\n",
    "  \n",
    "  for _ in range(epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "      hidden_layer = tf.matmul(x_train, W)\n",
    "      output_layer = tf.nn.softmax(tf.matmul(hidden_layer, V))\n",
    "      cross_entropy_loss = tf.reduce_mean(-tf.math.reduce_sum(y_train * tf.math.log(output_layer), axis=[1]))\n",
    "\n",
    "      grads = t.gradient(cross_entropy_loss, [W, V])\n",
    "      optimizer.apply_gradients(zip(grads,[W, V]))\n",
    "      if(_ % 1000 == 0):\n",
    "        print(cross_entropy_loss)\n",
    "\n",
    "\n",
    "\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.6123095, shape=(), dtype=float32)\n",
      "tf.Tensor(2.7801604, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4376032, shape=(), dtype=float32)\n",
      "tf.Tensor(2.2505875, shape=(), dtype=float32)\n",
      "tf.Tensor(2.1507685, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "word2idx, W, V = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79979108 0.33298967]\n",
      "[[0.16974153 0.56550604]\n",
      " [0.15930054 0.53072116]\n",
      " [0.00803347 0.02676408]\n",
      " [0.07554102 0.2516703 ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(6, 4)\n",
    "V = np.random.randn(4, 6)\n",
    "prob = sigmoid(W[3].dot(V[:,[2, 4]]))\n",
    "print(prob)\n",
    "\n",
    "gV = np.outer(W[3], prob - 1)\n",
    "print(gV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
