{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('../large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "  # returns 57340 of the Brown corpus\n",
    "  # each sentence is represented as a list of individual string tokens\n",
    "  return brown.sents()\n",
    "\n",
    "def get_brown(n_vocab=2000, keep_words = []):\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 0\n",
    "  word2idx = {}\n",
    "  idx2word = []\n",
    "\n",
    "  word_idx_count = {}\n",
    "\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        idx2word.append(token)\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      # keep track of counts for later sorting\n",
    "      idx = word2idx[token]\n",
    "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "      indexed_sentence.append(idx)\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "\n",
    "\n",
    "  # restrict vocab size\n",
    "\n",
    "  # set all the words I want to keep to infinity\n",
    "  # so that they are included when I pick the most\n",
    "  # common words\n",
    "  for word in keep_words:\n",
    "    word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  word2idx_small = {}\n",
    "  new_idx = 0\n",
    "  idx_new_idx_map = {}\n",
    "  for idx, count in sorted_word_idx_count[:n_vocab-1]:\n",
    "    word = idx2word[idx]\n",
    "    word2idx_small[word] = new_idx\n",
    "    idx_new_idx_map[idx] = new_idx\n",
    "    new_idx += 1\n",
    "  # let 'unknown' be the last token\n",
    "  word2idx_small['UNKNOWN'] = new_idx \n",
    "  unknown = new_idx\n",
    "\n",
    "  # map old idx to new idx\n",
    "  sentences_small = []\n",
    "  for sentence in indexed_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "      sentences_small.append(new_sentence)\n",
    "\n",
    "  return sentences_small, word2idx_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_sampling_distribution(sentences):\n",
    "  # Pn(w) = prob of word occuring\n",
    "  # we would like to sample the negative samples\n",
    "  # such that words that occur more often\n",
    "  # should be sampled more often\n",
    "\n",
    "  word_freq = {}\n",
    "  word_count = sum(len(sentence) for sentence in sentences)\n",
    "  for sentence in sentences:\n",
    "      for word in sentence:\n",
    "          if word not in word_freq:\n",
    "              word_freq[word] = 0\n",
    "          word_freq[word] += 1\n",
    "  \n",
    "  # vocab size\n",
    "  V = len(word_freq)\n",
    "\n",
    "  p_neg = np.zeros(V)\n",
    "  for j in range(V):\n",
    "      p_neg[j] = word_freq[j]**0.75\n",
    "\n",
    "  # normalize it\n",
    "  p_neg = p_neg / p_neg.sum()\n",
    "\n",
    "  assert(np.all(p_neg > 0))\n",
    "  return p_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(pos, sentence, window_size):\n",
    "  start = max(0, pos - window_size)\n",
    "  end_  = min(len(sentence), pos + window_size)\n",
    "\n",
    "  context = []\n",
    "  for ctx_pos, ctx_word_idx in enumerate(sentence[start:end_], start=start):\n",
    "    if ctx_pos != pos:\n",
    "      context.append(ctx_word_idx)\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset():\n",
    "    # get the data\n",
    "  sentences, word2idx = get_brown() #get_text8()\n",
    "\n",
    "  # number of unique words\n",
    "  vocab_size = len(word2idx)\n",
    "  \n",
    "  print(vocab_size)\n",
    "\n",
    "\n",
    "  # config\n",
    "  window_size = 10\n",
    "  learning_rate = 0.025\n",
    "  final_learning_rate = 0.0001\n",
    "  num_negatives = 5 # number of negative samples to draw per input word\n",
    "  samples_per_epoch = int(1e5)\n",
    "  epochs = 20\n",
    "  D = 50 # word embedding size\n",
    "\n",
    "  # learning rate decay\n",
    "  learning_rate_delta = (learning_rate - final_learning_rate) / epochs\n",
    "\n",
    "  # distribution for drawing negative samples\n",
    "  p_neg = get_negative_sampling_distribution(sentences)\n",
    "\n",
    "\n",
    "  # params\n",
    "\n",
    "  # biases = tf.Variable(np.zeros(vocab_size, dtype=np.float32))\n",
    "\n",
    "  def dot(A, B):\n",
    "    C = A * B\n",
    "    return tf.reduce_sum(input_tensor=C, axis=1)\n",
    "\n",
    "  # output = hidden.dot(tfV)\n",
    "\n",
    "  # loss\n",
    "  # neither of the built-in TF functions work well\n",
    "  # per_sample_loss = tf.nn.nce_loss(\n",
    "  # # per_sample_loss = tf.nn.sampled_softmax_loss(\n",
    "  #   weights=tfV,\n",
    "  #   biases=biases,\n",
    "  #   labels=tfY,\n",
    "  #   inputs=hidden,\n",
    "  #   num_sampled=num_negatives,\n",
    "  #   num_classes=vocab_size,\n",
    "  # )\n",
    "  # loss = tf.reduce_mean(per_sample_loss)\n",
    "\n",
    "  # optimizer\n",
    "  # train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  # train_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "  # save the costs to plot them per iteration\n",
    "  costs = []\n",
    "\n",
    "\n",
    "  # number of total words in corpus\n",
    "  total_words = sum(len(sentence) for sentence in sentences)\n",
    "  print(\"total number of words in corpus:\", total_words)\n",
    "\n",
    "\n",
    "  # for subsampling each sentence\n",
    "  threshold = 1e-5\n",
    "  p_drop = 1 - np.sqrt(threshold / p_neg)\n",
    "  \n",
    "  def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "  \n",
    "  inputs = []\n",
    "  contexts = []\n",
    "  labels = []\n",
    "\n",
    "  \n",
    "  for sentence in sentences:\n",
    "    # keep only certain words based on p_neg\n",
    "    sentence = [w for w in sentence \\\n",
    "      if np.random.random() < (1 - p_drop[w])\n",
    "    ]\n",
    "    if len(sentence) < 2:\n",
    "      continue\n",
    "\n",
    "\n",
    "    # randomly order words so we don't always see\n",
    "    # samples in the same order\n",
    "    randomly_ordered_positions = np.random.choice(\n",
    "      len(sentence),\n",
    "      # size=np.random.randint(1, len(sentence) + 1),\n",
    "      size=len(sentence),\n",
    "      replace=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    for j, pos in enumerate(randomly_ordered_positions):\n",
    "      # the middle word\n",
    "      word = sentence[pos]\n",
    "      context_words = get_context(pos, sentence, window_size)\n",
    "      neg_word = np.random.choice(vocab_size, p=p_neg)\n",
    "      for target in context_words:\n",
    "        inputs.append(to_one_hot(word, vocab_size))\n",
    "        contexts.append(to_one_hot(target, vocab_size))\n",
    "        labels.append(1)\n",
    "        inputs.append(to_one_hot(neg_word, vocab_size))\n",
    "        contexts.append(to_one_hot(target, vocab_size))\n",
    "        labels.append(0)\n",
    "        \n",
    "  inputs = np.asarray(inputs, dtype='float32')\n",
    "  contexts = np.asarray(contexts, dtype='float32')\n",
    "  labels = np.asarray(label, dtype='float32')\n",
    "  \n",
    "  return inputs, contexts, labels, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(savedir, inputs, contexts, labels, word2idx):\n",
    "  vocab_size = len(word2idx)\n",
    "  D = 50\n",
    "  W = tf.Variable(tf.random.normal([vocab_size, D]))\n",
    "  V = tf.Variable(tf.random.normal([D, vocab_size])) \n",
    "\n",
    "  optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "  def get_batch(batch_counter):\n",
    "    max_counter = len(label)\n",
    "    return inputs[batch_counter:min(batch_counter + 100, max_counter),], contexts[batch_counter:min(batch_counter + 100, max_counter),], labels[batch_counter:min(batch_counter + 100, max_counter)]\n",
    "\n",
    "  costs = []\n",
    "  \n",
    "  # train the model\n",
    "  for epoch in range(20):\n",
    "    cost = 0\n",
    "    batch_counter = 0\n",
    "    while batch_counter < len(label):\n",
    "      input_batch, context_batch, label_batch = get_batch(batch_counter)\n",
    "      batch_counter += 100\n",
    "      with tf.GradientTape() as t:\n",
    "        input_embeddings = tf.matmul(input_batch, W) # (100, 2000) * (2000, 50) - > (100, 50) embedding vector for each input\n",
    "        context_embeddings = tf.matmul(context_batch, tf.transpose(V)) # (100, 50)\n",
    "        simlarity = tf.einsum(\"ij,ij->i\", input_embeddings, context_embeddings)\n",
    "        \n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          labels=label_batch, logits=simlarity)\n",
    "        \n",
    "        cost += tf.reduce_sum(loss)\n",
    "\n",
    "        grads = t.gradient(loss, [W, V])\n",
    "        optimizer.apply_gradients(zip(grads,[W, V]))\n",
    "        \n",
    "        if batch_counter % 10000 == 0:\n",
    "          sys.stdout.write(\"processed %s / %s\\r\" % (batch_counter, len(labels)))\n",
    "          sys.stdout.flush()\n",
    "    \n",
    "    print(\"epoch complete:\", epoch, \"cost:\", cost)\n",
    "  # return the model\n",
    "  return word2idx, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "total number of words in corpus: 1160865\n",
      "(298790, 2000) (298790, 2000) (296072,)\n"
     ]
    }
   ],
   "source": [
    "inputs, contexts, labels, word2idx = construct_dataset()\n",
    "\n",
    "print(inputs.shape, contexts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch complete: 0 cost: tf.Tensor(630931.94, shape=(), dtype=float32)\n",
      "epoch complete: 1 cost: tf.Tensor(433847.6, shape=(), dtype=float32)\n",
      "epoch complete: 2 cost: tf.Tensor(349546.22, shape=(), dtype=float32)\n",
      "epoch complete: 3 cost: tf.Tensor(298306.97, shape=(), dtype=float32)\n",
      "epoch complete: 4 cost: tf.Tensor(263798.6, shape=(), dtype=float32)\n",
      "epoch complete: 5 cost: tf.Tensor(239087.58, shape=(), dtype=float32)\n",
      "epoch complete: 6 cost: tf.Tensor(220630.42, shape=(), dtype=float32)\n",
      "epoch complete: 7 cost: tf.Tensor(206410.08, shape=(), dtype=float32)\n",
      "epoch complete: 8 cost: tf.Tensor(195184.58, shape=(), dtype=float32)\n",
      "epoch complete: 9 cost: tf.Tensor(186145.83, shape=(), dtype=float32)\n",
      "epoch complete: 10 cost: tf.Tensor(178748.83, shape=(), dtype=float32)\n",
      "epoch complete: 11 cost: tf.Tensor(172608.73, shape=(), dtype=float32)\n",
      "epoch complete: 12 cost: tf.Tensor(167444.9, shape=(), dtype=float32)\n",
      "epoch complete: 13 cost: tf.Tensor(163050.58, shape=(), dtype=float32)\n",
      "epoch complete: 14 cost: tf.Tensor(159268.67, shape=(), dtype=float32)\n",
      "epoch complete: 15 cost: tf.Tensor(155980.42, shape=(), dtype=float32)\n",
      "epoch complete: 16 cost: tf.Tensor(153093.75, shape=(), dtype=float32)\n",
      "epoch complete: 17 cost: tf.Tensor(150536.25, shape=(), dtype=float32)\n",
      "epoch complete: 18 cost: tf.Tensor(148251.8, shape=(), dtype=float32)\n",
      "epoch complete: 19 cost: tf.Tensor(146193.73, shape=(), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(2000, 50) dtype=float32, numpy=\n",
      "array([[-0.3455732 , -0.03174401,  0.02158272, ...,  0.09234573,\n",
      "         0.09032178, -0.22473188],\n",
      "       [-0.06133434,  0.07464878, -0.06392034, ..., -0.02111472,\n",
      "         0.08687677, -0.04314757],\n",
      "       [-0.3760927 ,  0.00688569, -0.08449373, ...,  0.07774831,\n",
      "        -0.03719903,  0.00480525],\n",
      "       ...,\n",
      "       [ 0.5079636 , -0.38961694, -0.25100282, ..., -1.211722  ,\n",
      "        -0.8478748 , -1.7721817 ],\n",
      "       [-0.3447626 ,  0.00637528, -0.16311522, ...,  0.24958792,\n",
      "         1.5857931 , -0.4317737 ],\n",
      "       [-0.42255065,  0.03659458,  0.11579508, ...,  0.03455456,\n",
      "         0.06772879, -0.08121987]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "word2idx, W, V = train_model('w2v_tf2', inputs, contexts, labels, word2idx)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "  V, D = W.shape\n",
    "\n",
    "  # don't actually use pos2 in calculation, just print what's expected\n",
    "  print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "  for w in (pos1, neg1, pos2, neg2):\n",
    "    if w not in word2idx:\n",
    "      print(\"Sorry, %s not in word2idx\" % w)\n",
    "      return\n",
    "\n",
    "  p1 = W[word2idx[pos1]]\n",
    "  n1 = W[word2idx[neg1]]\n",
    "  p2 = W[word2idx[pos2]]\n",
    "  n2 = W[word2idx[neg2]]\n",
    "\n",
    "  vec = p1 - n1 + n2\n",
    "\n",
    "  distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "  idx = distances.argsort()[:10]\n",
    "\n",
    "  # pick one that's not p1, n1, or n2\n",
    "  best_idx = -1\n",
    "  keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    "  # print(\"keep_out:\", keep_out)\n",
    "  for i in idx:\n",
    "    if i not in keep_out:\n",
    "      best_idx = i\n",
    "      break\n",
    "  # print(\"best_idx:\", best_idx)\n",
    "\n",
    "  print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "  print(\"closest 10:\")\n",
    "  for i in idx:\n",
    "    print(idx2word[i], distances[i])\n",
    "\n",
    "  print(\"dist to %s:\" % pos2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3455732  -0.03174401  0.02158272 ...  0.09234573  0.09032178\n",
      "  -0.22473188]\n",
      " [-0.06133434  0.07464878 -0.06392034 ... -0.02111472  0.08687677\n",
      "  -0.04314757]\n",
      " [-0.3760927   0.00688569 -0.08449373 ...  0.07774831 -0.03719903\n",
      "   0.00480525]\n",
      " ...\n",
      " [ 0.5079636  -0.38961694 -0.25100282 ... -1.211722   -0.8478748\n",
      "  -1.7721817 ]\n",
      " [-0.3447626   0.00637528 -0.16311522 ...  0.24958792  1.5857931\n",
      "  -0.4317737 ]\n",
      " [-0.42255065  0.03659458  0.11579508 ...  0.03455456  0.06772879\n",
      "  -0.08121987]]\n",
      "**********\n",
      "testing: france - french = england - english\n",
      "got: france - french = nature - english\n",
      "closest 10:\n",
      "france 0.4186057\n",
      "nature 0.43574852\n",
      "along 0.5771315\n",
      "english 0.58281875\n",
      "left 0.59090513\n",
      "answered 0.6115551\n",
      "`` 0.6173\n",
      "taking 0.625888\n",
      "remember 0.6353825\n",
      "campaign 0.6405154\n",
      "dist to england: 0.9129335582256317\n",
      "**********\n",
      "testing: france - french = england - english\n",
      "got: france - french = better - english\n",
      "closest 10:\n",
      "france 0.3301431\n",
      "better 0.58228135\n",
      "greatly 0.6109736\n",
      "they 0.6158257\n",
      "1958 0.6187956\n",
      "remember 0.61980057\n",
      "effects 0.6299254\n",
      "arts 0.6454344\n",
      "text 0.6500225\n",
      "concept 0.6519505\n",
      "dist to england: 0.7012848854064941\n"
     ]
    }
   ],
   "source": [
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "print(W)\n",
    "for We in (W, (W + V.T) / 2):\n",
    "  print(\"**********\")\n",
    "\n",
    "  analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
