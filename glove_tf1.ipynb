{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('../large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "  return brown.sents()\n",
    "\n",
    "def get_brown(n_vocab=2000, keep_words = []):\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 0\n",
    "  word2idx = {}\n",
    "  idx2word = []\n",
    "\n",
    "  word_idx_count = {}\n",
    "\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        idx2word.append(token)\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      idx = word2idx[token]\n",
    "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "      indexed_sentence.append(idx)\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "  for word in keep_words:\n",
    "    word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  word2idx_small = {}\n",
    "  new_idx = 0\n",
    "  idx_new_idx_map = {}\n",
    "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "    word = idx2word[idx]\n",
    "    word2idx_small[word] = new_idx\n",
    "    idx_new_idx_map[idx] = new_idx\n",
    "    new_idx += 1\n",
    "  word2idx_small['UNKNOWN'] = new_idx \n",
    "  unknown = new_idx\n",
    "\n",
    "  sentences_small = []\n",
    "  for sentence in indexed_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "      sentences_small.append(new_sentence)\n",
    "\n",
    "  return sentences_small, word2idx_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def construct_matrix(cc_matrix, sentences, V, context_sz):\n",
    "  if not os.path.exists(cc_matrix):\n",
    "      X = np.zeros((V, V))\n",
    "      N = len(sentences)\n",
    "      print(\"number of sentences to process:\", N)\n",
    "      it = 0\n",
    "      for sentence in sentences:\n",
    "          it += 1\n",
    "          if it % 10000 == 0:\n",
    "              print(\"processed\", it, \"/\", N)\n",
    "          n = len(sentence)\n",
    "          for i in range(n):\n",
    "              wi = sentence[i]\n",
    "\n",
    "              start = max(0, i - context_sz)\n",
    "              end = min(n, i + context_sz)\n",
    "\n",
    "              if i - context_sz < 0:\n",
    "                  points = 1.0 / (i + 1)\n",
    "                  X[wi,0] += points\n",
    "                  X[0,wi] += points\n",
    "              if i + context_sz > n:\n",
    "                  points = 1.0 / (n - i)\n",
    "                  X[wi,1] += points\n",
    "                  X[1,wi] += points\n",
    "\n",
    "              for j in range(start, i):\n",
    "                  wj = sentence[j]\n",
    "                  points = 1.0 / (i - j) # this is +ve\n",
    "                  X[wi,wj] += points\n",
    "                  X[wj,wi] += points\n",
    "\n",
    "              for j in range(i + 1, end):\n",
    "                  wj = sentence[j]\n",
    "                  points = 1.0 / (j - i) # this is +ve\n",
    "                  X[wi,wj] += points\n",
    "                  X[wj,wi] += points\n",
    "\n",
    "      np.save(cc_matrix, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(cc_matrix, D, learning_rate=1e-5, reg=0.1, xmax=100, alpha=0.75, epochs=100):\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  X = np.load(cc_matrix)\n",
    "  V = len(X)\n",
    "  print(\"max in X:\", X.max())\n",
    "\n",
    "  # weighting\n",
    "  fX = np.zeros((V, V))\n",
    "  fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "  fX[X >= xmax] = 1\n",
    "\n",
    "  print(\"max in f(X):\", fX.max())\n",
    "\n",
    "  # target\n",
    "  logX = np.log(X + 1)\n",
    "\n",
    "  print(\"max in log(X):\", logX.max())\n",
    "\n",
    "  # initialize weights\n",
    "  W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "  b = np.zeros(V)\n",
    "  U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "  c = np.zeros(V)\n",
    "  mu = logX.mean()\n",
    "\n",
    "  # initialize weights, inputs, targets placeholders\n",
    "  tfW = tf.Variable(W.astype(np.float32))\n",
    "  tfb = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "  tfU = tf.Variable(U.astype(np.float32))\n",
    "  tfc = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
    "  tfLogX = tf.compat.v1.placeholder(tf.float32, shape=(V, V))\n",
    "  tffX = tf.compat.v1.placeholder(tf.float32, shape=(V, V))\n",
    "\n",
    "  delta = tf.matmul(tfW, tf.transpose(a=tfU)) + tfb + tfc + mu - tfLogX\n",
    "  cost = tf.reduce_sum(input_tensor=tffX * delta * delta)\n",
    "  regularized_cost = cost\n",
    "  for param in (tfW, tfU):\n",
    "      regularized_cost += reg*tf.reduce_sum(input_tensor=param * param)\n",
    "\n",
    "  train_op = tf.compat.v1.train.MomentumOptimizer(\n",
    "    learning_rate,\n",
    "    momentum=0.9\n",
    "  ).minimize(regularized_cost)\n",
    "  # train_op = tf.train.AdamOptimizer(1e-3).minimize(regularized_cost)\n",
    "  init = tf.compat.v1.global_variables_initializer()\n",
    "  session = tf.compat.v1.InteractiveSession()\n",
    "  session.run(init)\n",
    "\n",
    "  costs = []\n",
    "  for epoch in range(epochs):\n",
    "      c, _ = session.run((cost, train_op), feed_dict={tfLogX: logX, tffX: fX})\n",
    "      print(\"epoch:\", epoch, \"cost:\", c)\n",
    "      costs.append(c)\n",
    "\n",
    "  # save for future calculations\n",
    "  W, U = session.run([tfW, tfU])\n",
    "  return W, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_wiki()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max in X: 10554944.031889495\n",
      "max in f(X): 1.0\n",
      "max in log(X): 16.1721050314717\n",
      "WARNING:tensorflow:From C:\\Programs\\Miniconda3_64\\envs\\test\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "epoch: 0 cost: 48124384.0\n",
      "epoch: 1 cost: 35185476.0\n",
      "epoch: 2 cost: 22557570.0\n",
      "epoch: 3 cost: 18661240.0\n",
      "epoch: 4 cost: 19930980.0\n",
      "epoch: 5 cost: 19206512.0\n",
      "epoch: 6 cost: 15427730.0\n",
      "epoch: 7 cost: 12667851.0\n",
      "epoch: 8 cost: 12987077.0\n",
      "epoch: 9 cost: 14055838.0\n",
      "epoch: 10 cost: 13322813.0\n",
      "epoch: 11 cost: 11453230.0\n",
      "epoch: 12 cost: 10582043.0\n",
      "epoch: 13 cost: 10983856.0\n",
      "epoch: 14 cost: 11075684.0\n",
      "epoch: 15 cost: 10021385.0\n",
      "epoch: 16 cost: 8784889.0\n",
      "epoch: 17 cost: 8465030.0\n",
      "epoch: 18 cost: 8736060.0\n",
      "epoch: 19 cost: 8544645.0\n",
      "epoch: 20 cost: 7702225.0\n",
      "epoch: 21 cost: 7014430.0\n",
      "epoch: 22 cost: 7011961.0\n",
      "epoch: 23 cost: 7256971.0\n",
      "epoch: 24 cost: 7080768.0\n",
      "epoch: 25 cost: 6518613.5\n",
      "epoch: 26 cost: 6147890.5\n",
      "epoch: 27 cost: 6228239.0\n",
      "epoch: 28 cost: 6404731.0\n",
      "epoch: 29 cost: 6267929.0\n",
      "epoch: 30 cost: 5887027.0\n",
      "epoch: 31 cost: 5633744.5\n",
      "epoch: 32 cost: 5662200.5\n",
      "epoch: 33 cost: 5771857.0\n",
      "epoch: 34 cost: 5735365.5\n",
      "epoch: 35 cost: 5571557.0\n",
      "epoch: 36 cost: 5443903.5\n",
      "epoch: 37 cost: 5416963.5\n",
      "epoch: 38 cost: 5414530.5\n",
      "epoch: 39 cost: 5367349.0\n",
      "epoch: 40 cost: 5299762.0\n",
      "epoch: 41 cost: 5261491.0\n",
      "epoch: 42 cost: 5246672.0\n",
      "epoch: 43 cost: 5218652.5\n",
      "epoch: 44 cost: 5176521.5\n",
      "epoch: 45 cost: 5152716.5\n",
      "epoch: 46 cost: 5156068.0\n",
      "epoch: 47 cost: 5155499.5\n",
      "epoch: 48 cost: 5126240.0\n",
      "epoch: 49 cost: 5084761.5\n",
      "epoch: 50 cost: 5062944.0\n",
      "epoch: 51 cost: 5064272.5\n",
      "epoch: 52 cost: 5064389.5\n",
      "epoch: 53 cost: 5048332.0\n",
      "epoch: 54 cost: 5028136.5\n",
      "epoch: 55 cost: 5020675.5\n",
      "epoch: 56 cost: 5023069.0\n",
      "epoch: 57 cost: 5019660.5\n",
      "epoch: 58 cost: 5005563.5\n",
      "epoch: 59 cost: 4991794.0\n",
      "epoch: 60 cost: 4988045.0\n",
      "epoch: 61 cost: 4990044.0\n",
      "epoch: 62 cost: 4987509.0\n",
      "epoch: 63 cost: 4978358.5\n",
      "epoch: 64 cost: 4969425.0\n",
      "epoch: 65 cost: 4965696.5\n",
      "epoch: 66 cost: 4964653.5\n",
      "epoch: 67 cost: 4961728.5\n",
      "epoch: 68 cost: 4956811.5\n",
      "epoch: 69 cost: 4952808.5\n",
      "epoch: 70 cost: 4950333.0\n",
      "epoch: 71 cost: 4947043.5\n",
      "epoch: 72 cost: 4941760.5\n",
      "epoch: 73 cost: 4936404.0\n",
      "epoch: 74 cost: 4933017.0\n",
      "epoch: 75 cost: 4930717.0\n",
      "epoch: 76 cost: 4927002.0\n",
      "epoch: 77 cost: 4921210.0\n",
      "epoch: 78 cost: 4915113.0\n",
      "epoch: 79 cost: 4910186.0\n",
      "epoch: 80 cost: 4905645.0\n",
      "epoch: 81 cost: 4899727.0\n",
      "epoch: 82 cost: 4892036.0\n",
      "epoch: 83 cost: 4883722.5\n",
      "epoch: 84 cost: 4875673.0\n",
      "epoch: 85 cost: 4867392.0\n",
      "epoch: 86 cost: 4857863.5\n",
      "epoch: 87 cost: 4846856.0\n",
      "epoch: 88 cost: 4834909.5\n",
      "epoch: 89 cost: 4822359.0\n",
      "epoch: 90 cost: 4808883.5\n",
      "epoch: 91 cost: 4794049.5\n",
      "epoch: 92 cost: 4777890.5\n",
      "epoch: 93 cost: 4760741.0\n",
      "epoch: 94 cost: 4742723.5\n",
      "epoch: 95 cost: 4723688.0\n",
      "epoch: 96 cost: 4703573.0\n",
      "epoch: 97 cost: 4682635.0\n",
      "epoch: 98 cost: 4661243.5\n",
      "epoch: 99 cost: 4639609.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# construct_matrix('cc_matrix_wiki', sentences, V, 10)\n",
    "W, U = train('cc_matrix_wiki.npy', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3, We, word2idx, idx2word):\n",
    "    V, D = We.shape\n",
    "\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "\n",
    "    for dist in ('euclidean', 'cosine'):\n",
    "        distances = pairwise_distances(v0.reshape(1, D), We, metric=dist).reshape(V)\n",
    "        # idx = distances.argmin()\n",
    "        # best_word = idx2word[idx]\n",
    "        idx = distances.argsort()[:4]\n",
    "        best_idx = -1\n",
    "        keep_out = [word2idx[w] for w in (w1, w2, w3)]\n",
    "        for i in idx:\n",
    "            if i not in keep_out:\n",
    "                best_idx = i\n",
    "                break\n",
    "        best_word = idx2word[best_idx]\n",
    "\n",
    "\n",
    "        print(\"closest match by\", dist, \"distance:\", best_word)\n",
    "        print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 100) (20000, 100)\n",
      "closest match by euclidean distance: charles\n",
      "king - man = charles - woman\n",
      "closest match by cosine distance: ii\n",
      "king - man = ii - woman\n",
      "closest match by euclidean distance: europe\n",
      "france - paris = europe - london\n",
      "closest match by cosine distance: united\n",
      "france - paris = united - london\n",
      "closest match by euclidean distance: england\n",
      "france - paris = england - rome\n",
      "closest match by cosine distance: war\n",
      "france - paris = war - rome\n",
      "closest match by euclidean distance: ceased\n",
      "paris - france = ceased - italy\n",
      "closest match by cosine distance: runs\n",
      "paris - france = runs - italy\n",
      "closest match by euclidean distance: america\n",
      "france - french = america - english\n",
      "closest match by cosine distance: europe\n",
      "france - french = europe - english\n",
      "closest match by euclidean distance: australia\n",
      "japan - japanese = australia - chinese\n",
      "closest match by cosine distance: south\n",
      "japan - japanese = south - chinese\n",
      "closest match by euclidean distance: decline\n",
      "japan - japanese = decline - italian\n",
      "closest match by cosine distance: american\n",
      "japan - japanese = american - italian\n",
      "closest match by euclidean distance: conference\n",
      "japan - japanese = conference - australian\n",
      "closest match by cosine distance: 1989\n",
      "japan - japanese = 1989 - australian\n",
      "closest match by euclidean distance: march\n",
      "december - november = march - june\n",
      "closest match by cosine distance: march\n",
      "december - november = march - june\n"
     ]
    }
   ],
   "source": [
    "print(W.shape, U.shape)\n",
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "We = (W + U) / 2\n",
    "\n",
    "find_analogies('king', 'man', 'woman', We, word2idx, idx2word)\n",
    "find_analogies('france', 'paris', 'london', We, word2idx, idx2word)\n",
    "find_analogies('france', 'paris', 'rome', We, word2idx, idx2word)\n",
    "find_analogies('paris', 'france', 'italy', We, word2idx, idx2word)\n",
    "find_analogies('france', 'french', 'english', We, word2idx, idx2word)\n",
    "find_analogies('japan', 'japanese', 'chinese', We, word2idx, idx2word)\n",
    "find_analogies('japan', 'japanese', 'italian', We, word2idx, idx2word)\n",
    "find_analogies('japan', 'japanese', 'australian', We, word2idx, idx2word)\n",
    "find_analogies('december', 'november', 'june', We, word2idx, idx2word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
