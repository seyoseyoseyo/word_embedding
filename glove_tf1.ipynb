{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import json\n",
    "\n",
    "\n",
    "from nltk.corpus import brown\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine as cos_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "  return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki():\n",
    "  V = 20000\n",
    "  files = glob('../large_files/enwiki*.txt')\n",
    "  all_word_counts = {}\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          for word in s:\n",
    "            if word not in all_word_counts:\n",
    "              all_word_counts[word] = 0\n",
    "            all_word_counts[word] += 1\n",
    "  print(\"finished counting\")\n",
    "\n",
    "  V = min(V, len(all_word_counts))\n",
    "  all_word_counts = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  top_words = [w for w, count in all_word_counts[:V-1]] + ['<UNK>']\n",
    "  word2idx = {w:i for i, w in enumerate(top_words)}\n",
    "  unk = word2idx['<UNK>']\n",
    "\n",
    "  sents = []\n",
    "  for f in files:\n",
    "    for line in open(f, encoding=\"utf8\"):\n",
    "      if line and line[0] not in '[*-|=\\{\\}':\n",
    "        s = remove_punctuation(line).lower().split()\n",
    "        if len(s) > 1:\n",
    "          # if a word is not nearby another word, there won't be any context!\n",
    "          # and hence nothing to train!\n",
    "          sent = [word2idx[w] if w in word2idx else unk for w in s]\n",
    "          sents.append(sent)\n",
    "  return sents, word2idx\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "  return brown.sents()\n",
    "\n",
    "def get_brown(n_vocab=2000, keep_words = []):\n",
    "  sentences = get_sentences()\n",
    "  indexed_sentences = []\n",
    "\n",
    "  i = 0\n",
    "  word2idx = {}\n",
    "  idx2word = []\n",
    "\n",
    "  word_idx_count = {}\n",
    "\n",
    "  for sentence in sentences:\n",
    "    indexed_sentence = []\n",
    "    for token in sentence:\n",
    "      token = token.lower()\n",
    "      if token not in word2idx:\n",
    "        idx2word.append(token)\n",
    "        word2idx[token] = i\n",
    "        i += 1\n",
    "\n",
    "      idx = word2idx[token]\n",
    "      word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "\n",
    "      indexed_sentence.append(idx)\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "  for word in keep_words:\n",
    "    word_idx_count[word2idx[word]] = float('inf')\n",
    "\n",
    "  sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "  word2idx_small = {}\n",
    "  new_idx = 0\n",
    "  idx_new_idx_map = {}\n",
    "  for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "    word = idx2word[idx]\n",
    "    word2idx_small[word] = new_idx\n",
    "    idx_new_idx_map[idx] = new_idx\n",
    "    new_idx += 1\n",
    "  word2idx_small['UNKNOWN'] = new_idx \n",
    "  unknown = new_idx\n",
    "\n",
    "  sentences_small = []\n",
    "  for sentence in indexed_sentences:\n",
    "    if len(sentence) > 1:\n",
    "      new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "      sentences_small.append(new_sentence)\n",
    "\n",
    "  return sentences_small, word2idx_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix(cc_matrix, sentences, V, context_sz):\n",
    "  if not os.path.exists(cc_matrix):\n",
    "      X = np.zeros((V, V))\n",
    "      N = len(sentences)\n",
    "      print(\"number of sentences to process:\", N)\n",
    "      it = 0\n",
    "      for sentence in sentences:\n",
    "          it += 1\n",
    "          if it % 10000 == 0:\n",
    "              print(\"processed\", it, \"/\", N)\n",
    "          n = len(sentence)\n",
    "          for i in range(n):\n",
    "              wi = sentence[i]\n",
    "\n",
    "              start = max(0, i - context_sz)\n",
    "              end = min(n, i + context_sz)\n",
    "\n",
    "              if i - context_sz < 0:\n",
    "                  points = 1.0 / (i + 1)\n",
    "                  X[wi,0] += points\n",
    "                  X[0,wi] += points\n",
    "              if i + context_sz > n:\n",
    "                  points = 1.0 / (n - i)\n",
    "                  X[wi,1] += points\n",
    "                  X[1,wi] += points\n",
    "\n",
    "              for j in range(start, i):\n",
    "                  wj = sentence[j]\n",
    "                  points = 1.0 / (i - j) # this is +ve\n",
    "                  X[wi,wj] += points\n",
    "                  X[wj,wi] += points\n",
    "\n",
    "              for j in range(i + 1, end):\n",
    "                  wj = sentence[j]\n",
    "                  points = 1.0 / (j - i) # this is +ve\n",
    "                  X[wi,wj] += points\n",
    "                  X[wj,wi] += points\n",
    "\n",
    "      np.save(cc_matrix, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cc_matrix, V, D, learning_rate=1e-4, reg=0.1, xmax=100, alpha=0.75, epochs=100):\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  X = np.load(cc_matrix)\n",
    "  print(\"max in X:\", X.max())\n",
    "\n",
    "  # weighting\n",
    "  fX = np.zeros((V, V))\n",
    "  fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "  fX[X >= xmax] = 1\n",
    "\n",
    "  print(\"max in f(X):\", fX.max())\n",
    "\n",
    "  # target\n",
    "  logX = np.log(X + 1)\n",
    "\n",
    "  print(\"max in log(X):\", logX.max())\n",
    "\n",
    "  # initialize weights\n",
    "  W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "  b = np.zeros(V)\n",
    "  U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "  c = np.zeros(V)\n",
    "  mu = logX.mean()\n",
    "\n",
    "  # initialize weights, inputs, targets placeholders\n",
    "  tfW = tf.Variable(W.astype(np.float32))\n",
    "  tfb = tf.Variable(b.reshape(V, 1).astype(np.float32))\n",
    "  tfU = tf.Variable(U.astype(np.float32))\n",
    "  tfc = tf.Variable(c.reshape(1, V).astype(np.float32))\n",
    "  tfLogX = tf.compat.v1.placeholder(tf.float32, shape=(V, V))\n",
    "  tffX = tf.compat.v1.placeholder(tf.float32, shape=(V, V))\n",
    "\n",
    "  delta = tf.matmul(tfW, tf.transpose(a=tfU)) + tfb + tfc + mu - tfLogX\n",
    "  cost = tf.reduce_sum(input_tensor=tffX * delta * delta)\n",
    "  regularized_cost = cost\n",
    "  for param in (tfW, tfU):\n",
    "      regularized_cost += reg*tf.reduce_sum(input_tensor=param * param)\n",
    "\n",
    "  train_op = tf.compat.v1.train.MomentumOptimizer(\n",
    "    learning_rate,\n",
    "    momentum=0.9\n",
    "  ).minimize(regularized_cost)\n",
    "  # train_op = tf.train.AdamOptimizer(1e-3).minimize(regularized_cost)\n",
    "  init = tf.compat.v1.global_variables_initializer()\n",
    "  session = tf.compat.v1.InteractiveSession()\n",
    "  session.run(init)\n",
    "\n",
    "  costs = []\n",
    "  sentence_indexes = range(len(sentences))\n",
    "  for epoch in range(epochs):\n",
    "      c, _ = session.run((cost, train_op), feed_dict={tfLogX: logX, tffX: fX})\n",
    "      print(\"epoch:\", epoch, \"cost:\", c)\n",
    "      costs.append(c)\n",
    "\n",
    "  # save for future calculations\n",
    "  W, U = session.run([tfW, tfU])\n",
    "  return W, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max in X: 502704.6412708048\n",
      "max in f(X): 1.0\n",
      "max in log(X): 13.127760071560818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost: 716149.8\n",
      "epoch: 1 cost: 468802.97\n",
      "epoch: 2 cost: 236526.61\n",
      "epoch: 3 cost: 188934.94\n",
      "epoch: 4 cost: 254416.97\n",
      "epoch: 5 cost: 278433.38\n",
      "epoch: 6 cost: 222999.83\n",
      "epoch: 7 cost: 165718.36\n",
      "epoch: 8 cost: 162992.72\n",
      "epoch: 9 cost: 185571.17\n",
      "epoch: 10 cost: 183009.48\n",
      "epoch: 11 cost: 155528.08\n",
      "epoch: 12 cost: 137868.7\n",
      "epoch: 13 cost: 141035.84\n",
      "epoch: 14 cost: 141300.84\n",
      "epoch: 15 cost: 120410.695\n",
      "epoch: 16 cost: 91344.016\n",
      "epoch: 17 cost: 77912.734\n",
      "epoch: 18 cost: 84179.3\n",
      "epoch: 19 cost: 94306.58\n",
      "epoch: 20 cost: 94951.695\n",
      "epoch: 21 cost: 87087.59\n",
      "epoch: 22 cost: 77832.484\n",
      "epoch: 23 cost: 70386.38\n",
      "epoch: 24 cost: 64786.543\n",
      "epoch: 25 cost: 62155.438\n",
      "epoch: 26 cost: 63125.484\n",
      "epoch: 27 cost: 64530.125\n",
      "epoch: 28 cost: 62164.336\n",
      "epoch: 29 cost: 56565.125\n",
      "epoch: 30 cost: 52702.24\n",
      "epoch: 31 cost: 53319.773\n",
      "epoch: 32 cost: 55320.875\n",
      "epoch: 33 cost: 54212.516\n",
      "epoch: 34 cost: 50115.098\n",
      "epoch: 35 cost: 47107.215\n",
      "epoch: 36 cost: 47439.613\n",
      "epoch: 37 cost: 48873.355\n",
      "epoch: 38 cost: 48237.816\n",
      "epoch: 39 cost: 45663.418\n",
      "epoch: 40 cost: 43856.625\n",
      "epoch: 41 cost: 44152.477\n",
      "epoch: 42 cost: 45016.74\n",
      "epoch: 43 cost: 44521.742\n",
      "epoch: 44 cost: 42879.777\n",
      "epoch: 45 cost: 41745.65\n",
      "epoch: 46 cost: 41802.574\n",
      "epoch: 47 cost: 42083.363\n",
      "epoch: 48 cost: 41556.906\n",
      "epoch: 49 cost: 40468.44\n",
      "epoch: 50 cost: 39750.484\n",
      "epoch: 51 cost: 39670.324\n",
      "epoch: 52 cost: 39626.83\n",
      "epoch: 53 cost: 39111.79\n",
      "epoch: 54 cost: 38336.113\n",
      "epoch: 55 cost: 37785.812\n",
      "epoch: 56 cost: 37529.19\n",
      "epoch: 57 cost: 37235.855\n",
      "epoch: 58 cost: 36716.742\n",
      "epoch: 59 cost: 36151.42\n",
      "epoch: 60 cost: 35771.26\n",
      "epoch: 61 cost: 35542.41\n",
      "epoch: 62 cost: 35272.816\n",
      "epoch: 63 cost: 34906.65\n",
      "epoch: 64 cost: 34569.242\n",
      "epoch: 65 cost: 34355.297\n",
      "epoch: 66 cost: 34200.535\n",
      "epoch: 67 cost: 33993.246\n",
      "epoch: 68 cost: 33726.63\n",
      "epoch: 69 cost: 33480.496\n",
      "epoch: 70 cost: 33294.8\n",
      "epoch: 71 cost: 33127.29\n",
      "epoch: 72 cost: 32930.438\n",
      "epoch: 73 cost: 32714.246\n",
      "epoch: 74 cost: 32512.564\n",
      "epoch: 75 cost: 32323.209\n",
      "epoch: 76 cost: 32114.574\n",
      "epoch: 77 cost: 31878.697\n",
      "epoch: 78 cost: 31646.64\n",
      "epoch: 79 cost: 31447.488\n",
      "epoch: 80 cost: 31273.525\n",
      "epoch: 81 cost: 31096.713\n",
      "epoch: 82 cost: 30906.125\n",
      "epoch: 83 cost: 30714.947\n",
      "epoch: 84 cost: 30536.19\n",
      "epoch: 85 cost: 30366.168\n",
      "epoch: 86 cost: 30194.25\n",
      "epoch: 87 cost: 30017.836\n",
      "epoch: 88 cost: 29840.83\n",
      "epoch: 89 cost: 29663.285\n",
      "epoch: 90 cost: 29480.885\n",
      "epoch: 91 cost: 29293.668\n",
      "epoch: 92 cost: 29108.375\n",
      "epoch: 93 cost: 28929.64\n",
      "epoch: 94 cost: 28753.459\n",
      "epoch: 95 cost: 28572.77\n",
      "epoch: 96 cost: 28387.2\n",
      "epoch: 97 cost: 28203.443\n",
      "epoch: 98 cost: 28026.5\n",
      "epoch: 99 cost: 27854.195\n"
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_brown()\n",
    "V = len(word2idx)\n",
    "# construct_matrix('cc_matrix_brown', sentences, V, 10)\n",
    "W, U = train('cc_matrix_brown.npy', V, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished counting\n",
      "number of sentences to process: 1271558\n",
      "processed 10000 / 1271558\n",
      "processed 20000 / 1271558\n",
      "processed 30000 / 1271558\n",
      "processed 40000 / 1271558\n",
      "processed 50000 / 1271558\n",
      "processed 60000 / 1271558\n",
      "processed 70000 / 1271558\n",
      "processed 80000 / 1271558\n",
      "processed 90000 / 1271558\n",
      "processed 100000 / 1271558\n",
      "processed 110000 / 1271558\n",
      "processed 120000 / 1271558\n",
      "processed 130000 / 1271558\n",
      "processed 140000 / 1271558\n",
      "processed 150000 / 1271558\n",
      "processed 160000 / 1271558\n",
      "processed 170000 / 1271558\n",
      "processed 180000 / 1271558\n",
      "processed 190000 / 1271558\n",
      "processed 200000 / 1271558\n",
      "processed 210000 / 1271558\n",
      "processed 220000 / 1271558\n",
      "processed 230000 / 1271558\n",
      "processed 240000 / 1271558\n",
      "processed 250000 / 1271558\n",
      "processed 260000 / 1271558\n",
      "processed 270000 / 1271558\n",
      "processed 280000 / 1271558\n",
      "processed 290000 / 1271558\n",
      "processed 300000 / 1271558\n",
      "processed 310000 / 1271558\n",
      "processed 320000 / 1271558\n",
      "processed 330000 / 1271558\n",
      "processed 340000 / 1271558\n",
      "processed 350000 / 1271558\n",
      "processed 360000 / 1271558\n",
      "processed 370000 / 1271558\n",
      "processed 380000 / 1271558\n",
      "processed 390000 / 1271558\n",
      "processed 400000 / 1271558\n",
      "processed 410000 / 1271558\n",
      "processed 420000 / 1271558\n",
      "processed 430000 / 1271558\n",
      "processed 440000 / 1271558\n",
      "processed 450000 / 1271558\n",
      "processed 460000 / 1271558\n",
      "processed 470000 / 1271558\n",
      "processed 480000 / 1271558\n",
      "processed 490000 / 1271558\n",
      "processed 500000 / 1271558\n",
      "processed 510000 / 1271558\n",
      "processed 520000 / 1271558\n",
      "processed 530000 / 1271558\n",
      "processed 540000 / 1271558\n",
      "processed 550000 / 1271558\n",
      "processed 560000 / 1271558\n",
      "processed 570000 / 1271558\n",
      "processed 580000 / 1271558\n",
      "processed 590000 / 1271558\n",
      "processed 600000 / 1271558\n",
      "processed 610000 / 1271558\n",
      "processed 620000 / 1271558\n",
      "processed 630000 / 1271558\n",
      "processed 640000 / 1271558\n",
      "processed 650000 / 1271558\n",
      "processed 660000 / 1271558\n",
      "processed 670000 / 1271558\n",
      "processed 680000 / 1271558\n",
      "processed 690000 / 1271558\n",
      "processed 700000 / 1271558\n",
      "processed 710000 / 1271558\n",
      "processed 720000 / 1271558\n",
      "processed 730000 / 1271558\n",
      "processed 740000 / 1271558\n",
      "processed 750000 / 1271558\n",
      "processed 760000 / 1271558\n",
      "processed 770000 / 1271558\n",
      "processed 780000 / 1271558\n",
      "processed 790000 / 1271558\n",
      "processed 800000 / 1271558\n",
      "processed 810000 / 1271558\n",
      "processed 820000 / 1271558\n",
      "processed 830000 / 1271558\n",
      "processed 840000 / 1271558\n",
      "processed 850000 / 1271558\n",
      "processed 860000 / 1271558\n",
      "processed 870000 / 1271558\n",
      "processed 880000 / 1271558\n",
      "processed 890000 / 1271558\n",
      "processed 900000 / 1271558\n",
      "processed 910000 / 1271558\n",
      "processed 920000 / 1271558\n",
      "processed 930000 / 1271558\n",
      "processed 940000 / 1271558\n",
      "processed 950000 / 1271558\n",
      "processed 960000 / 1271558\n",
      "processed 970000 / 1271558\n",
      "processed 980000 / 1271558\n",
      "processed 990000 / 1271558\n",
      "processed 1000000 / 1271558\n",
      "processed 1010000 / 1271558\n",
      "processed 1020000 / 1271558\n",
      "processed 1030000 / 1271558\n",
      "processed 1040000 / 1271558\n",
      "processed 1050000 / 1271558\n",
      "processed 1060000 / 1271558\n",
      "processed 1070000 / 1271558\n",
      "processed 1080000 / 1271558\n",
      "processed 1090000 / 1271558\n",
      "processed 1100000 / 1271558\n",
      "processed 1110000 / 1271558\n",
      "processed 1120000 / 1271558\n",
      "processed 1130000 / 1271558\n",
      "processed 1140000 / 1271558\n",
      "processed 1150000 / 1271558\n",
      "processed 1160000 / 1271558\n",
      "processed 1170000 / 1271558\n",
      "processed 1180000 / 1271558\n",
      "processed 1190000 / 1271558\n",
      "processed 1200000 / 1271558\n",
      "processed 1210000 / 1271558\n",
      "processed 1220000 / 1271558\n",
      "processed 1230000 / 1271558\n",
      "processed 1240000 / 1271558\n",
      "processed 1250000 / 1271558\n",
      "processed 1260000 / 1271558\n",
      "processed 1270000 / 1271558\n",
      "max in X: 10554944.031889495\n",
      "max in f(X): 1.0\n",
      "max in log(X): 16.1721050314717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost: 48124376.0\n",
      "epoch: 1 cost: 107440690.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ef34cb11be40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconstruct_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cc_matrix_wiki'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cc_matrix_wiki.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-3675ef627776>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cc_matrix, V, D, learning_rate, reg, xmax, alpha, epochs)\u001b[0m\n\u001b[0;32m     49\u001b[0m   \u001b[0msentence_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m       \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mtfLogX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlogX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtffX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfX\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cost:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m       \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1181\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1182\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mazic\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences, word2idx = get_wiki()\n",
    "V = len(word2idx)\n",
    "construct_matrix('cc_matrix_wiki', sentences, V, 10)\n",
    "W, U = train('cc_matrix_wiki.npy', V, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(pos1, neg1, pos2, neg2, word2idx, idx2word, W):\n",
    "  V, D = W.shape\n",
    "\n",
    "  # don't actually use pos2 in calculation, just print what's expected\n",
    "  print(\"testing: %s - %s = %s - %s\" % (pos1, neg1, pos2, neg2))\n",
    "  for w in (pos1, neg1, pos2, neg2):\n",
    "    if w not in word2idx:\n",
    "      print(\"Sorry, %s not in word2idx\" % w)\n",
    "      return\n",
    "\n",
    "  p1 = W[word2idx[pos1]]\n",
    "  n1 = W[word2idx[neg1]]\n",
    "  p2 = W[word2idx[pos2]]\n",
    "  n2 = W[word2idx[neg2]]\n",
    "\n",
    "  vec = p1 - n1 + n2\n",
    "\n",
    "  distances = pairwise_distances(vec.reshape(1, D), W, metric='cosine').reshape(V)\n",
    "  idx = distances.argsort()[:10]\n",
    "\n",
    "  # pick one that's not p1, n1, or n2\n",
    "  best_idx = -1\n",
    "  keep_out = [word2idx[w] for w in (pos1, neg1, neg2)]\n",
    "  # print(\"keep_out:\", keep_out)\n",
    "  for i in idx:\n",
    "    if i not in keep_out:\n",
    "      best_idx = i\n",
    "      break\n",
    "  # print(\"best_idx:\", best_idx)\n",
    "\n",
    "  print(\"got: %s - %s = %s - %s\" % (pos1, neg1, idx2word[best_idx], neg2))\n",
    "  print(\"closest 10:\")\n",
    "  for i in idx:\n",
    "    print(idx2word[i], distances[i])\n",
    "\n",
    "  print(\"dist to %s:\" % pos2, cos_dist(p2, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 100) (2001, 100)\n",
      "testing: france - french = england - english\n",
      "got: france - french = interests - english\n",
      "closest 10:\n",
      "english 0.38648582\n",
      "france 0.4389286\n",
      "interests 0.5164262\n",
      "features 0.5265418\n",
      "advantage 0.53877854\n",
      "cells 0.5425768\n",
      "actual 0.5515337\n",
      "energy 0.55583745\n",
      "actions 0.567492\n",
      "painting 0.57363844\n",
      "dist to england: 0.9251848310232162\n"
     ]
    }
   ],
   "source": [
    "print(W.shape, U.shape)\n",
    "idx2word = {i:w for w, i in word2idx.items()}\n",
    "We = (W + U) / 2\n",
    "\n",
    "analogy('france', 'french', 'england', 'english', word2idx, idx2word, We)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
